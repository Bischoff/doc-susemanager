<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook50-profile.xsl"
                 type="text/xml" 
                 title="Profiling step"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude"
    xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="bp.chapt.suma3.troubleshooting">
    <title>Troubleshooting</title>

    <sect1 xml:id="bp.chapt.suma3.troubleshooting.registering.cloned.salt.systems">
        <title>Registering Cloned Salt Systems (minions)</title>

        <para>This chapter provides guidance on registering cloned systems with SUSE Manager. This
            includes both Salt and Traditional clients.</para>

        <procedure>
            <title>Registering a Cloned System with SUSE Manager (Salt and Traditional)</title>
            <step>
                <para> Clone your system (for example using the existing cloning mechanism of your
                    favorite Hypervisor)</para>
                <note>
                    <title>Quick Tips</title>
                    <para>Each step in this section is performed on the cloned system, this
                        procedure does not manipulate the original system, which will still be
                        registered to SUSE Manager. The cloned virtual machine should have a
                        different UUID from the original (this UUID is generated by your hypervisor)
                        or SUSE Manager will overwrite the original system data with the new
                        one.</para>
                </note>
            </step>
            <step>
                <para>Change the Hostname and IP addresses, also make sure /etc/hosts contains the
                    changes you made and the correct host entries.</para>
            </step>
            <step>
                <para>Stop rhnsd daemon with:</para>
                <screen># /etc/init.d/rhnsd stop </screen>
                <para>or alternativly:</para>
                <screen># rcrhnsd stop</screen>
            </step>
            <step>
                <para>Stop osad with:</para>
                <screen># /etc/init.d/osad stop</screen>
                <para>or alternativly:</para>
                <screen># rcosad stop</screen>
            </step>
            <step>
                <para>Remove the osad authentifcation configuration file and the systemid
                    with:</para>
                <screen># rm -f /etc/sysconfig/rhn/{osad-auth.conf,systemid}</screen>
            </step>

        </procedure>

        <para>The next step you take will depend on the Operating System of the clone.</para>
        <para>In case the following scenario occurs: If you see only one of your cloned Salt minions
            on the System Overview page after having accepted all minions keys from the Salt
            Onboarding page, then this is likely due to these machines being clones of the original
            and utilizing a duplicate machine-id. Perform the following steps to resolve this
            conflict.</para>

        <procedure>
            <title>Registering Salt Clones: SLES 12</title>
            <step>
                <para>SLES 12: If your machines have the same machine ids then delete the file on
                    each minion and recreate it:</para>
                <screen># cat /etc/machine-id
# rm /etc/machine-id
# systemd-machine-id-setup</screen>
            </step>
        </procedure>

        <procedure>
            <title>Registering Salt Clones: SLES 11</title>
            <step>
                <para>SLES 11: As there is no systemd machine id, generate one from dbus:</para>
                <screen># cat /var/lib/dbus/machine-id
# rm /var/lib/dbus/machine-id
# dbus-uuidgen --ensure</screen>
            </step>
        </procedure>

        <para> If your machines further have the same minion id then delete the minion_id file on
            each minion (FQDN will be used when it is regenerated on minion restart):</para>
        <screen># rm /etc/salt/minion_id</screen>
        <para>Finally delete accepted keys from Onboarding page and system profile from SUSE
            Manager, and restart the minion with:</para>
        <screen># systemctl restart salt-minion</screen>
        <para> You should be able to re-register them again, but each minion will use a different
            '/etc/machine-id' and should now be correctly displayed on the System Overview page. </para>

    </sect1>
    <sect1 xml:id="bp.chapt.suma3.troubleshooting.registering.cloned.traditional.systems">
        <title>Registering Cloned Traditional Systems</title>
        <para> This section provides guidance on registering cloned traditional systems which are
            registered via bootstrap. </para>

        <procedure>
            <title>Registering A Cloned Traditional System: SLES 12</title>
            <step>
                <para>Remove the following credential files:</para>
                <screen># rm  -f /etc/zypp/credentials.d/{SCCcredentials,NCCcredentials}</screen>
            </step>
            <step>
                <para>Re-run the bootstrap script. You should now see the cloned system in SUSE
                    Manager without overwriting the system it was cloned from.</para>
            </step>

        </procedure>

        <procedure>
            <title>Registering A Cloned Traditional System: SLES 11</title>
            <step>
                <para>Continued from section 1 step 5:</para>
                <screen># suse_register -E</screen>
                <para> (--erase-local-regdata, Erase all local files created from a previous
                    executed registration. This option make the system look like never registered)
                </para>
            </step>
            <step>
                <para>Re-run the bootstrap script. You should now see the cloned system in SUSE
                    Manager without overwriting the system it was cloned from.</para>
            </step>
            <step>
                <para> Re-run the bootstrap script. You should now see the cloned system in SUSE
                    Manager without overwriting the system it was cloned from. </para>
            </step>

        </procedure>

        <procedure>
            <title>Registering A Cloned Traditional System: SLES 10</title>
            <step>
                <para>Continued from section 1 step 5:</para>
                <screen># rm -rf /etc/{zmd,zypp}</screen>

            </step>
            <step>
                <screen># rm -rf /var/lib/zypp/  # ¡¡¡¡¡ except /var/lib/zypp/db/products/ !!!!!</screen>
            </step>
            <step>
                <screen># rm -rf /var/lib/zmd/</screen>
            </step>
            <step>
                <para>Re-run the bootstrap script. You should now see the cloned system in SUSE
                    Manager without overwriting the system it was cloned from.</para>
            </step>

        </procedure>

        <procedure>
            <title>RHEL 5,6 and 7</title>
            <step>
                <para>Continued from section 1 step 5:</para>
                <screen># rm  -f /etc/NCCcredentials</screen>
            </step>
            <step>
                <para>Re-run the bootstrap script. You should now see the cloned system in SUSE
                    Manager without overwriting the system it was cloned from.</para>
            </step>
        </procedure>

    </sect1>

    <sect1 xml:id="bp.chapt.suma3.troubleshooting.osad.jabberd">

        <title>Typical OSAD/jabberd Challenges</title>
        <para> This section provides answers for typical issues regarding OSAD and jabberd. </para>

        <sect2>
            <title>Open File Count Exceeded</title>
            <para><literal>SYMPTOMS</literal>: OSAD clients cannot contact the SUSE Manager Server,
                and jabberd requires long periods of time to respond on port 5222.</para>

            <para><literal>CAUSE</literal>: The number of maximum files that a jabber user can open
                is lower than the number of connected clients. Each client requires one permanently
                open TCP connection and each connection requires one file handler. The result is
                jabberd begins to queue and refuse connections.</para>

            <para><literal>CURE</literal>: Edit the <filename>/etc/security/limits.conf</filename>
                to something similar to the following:
                <screen>jabbersoftnofile&lt;#clients + 100&gt;
jabberhardnofile&lt;#clients + 1000&gt;</screen>
            </para>

            <para>This will vary according to your setup. For example in the case of 5000 clients:
                <screen>jabbersoftnofile5100
jabberhardnofile6000</screen></para>

            <para> Ensure you update the <filename>/etc/jabberd/c2s.xml</filename> max_fds parameter
                as well. For example: <screen>&lt;max_fds&gt;6000&lt;/max_fds&gt;</screen></para>

            <para><literal>EXPLANATION</literal>: The soft file limit is the limit of the maximum
                number of open files for a single process. In SUSE Manager the highest consuming
                process is c2s, which opens a connection per client. 100 additional files are added,
                here, to accommodate for any non-connection file that c2s requires to work
                correctly. The hard limit applies to all processes belonging to the jabber user, and
                accounts for open files from the router, s2s and sm processes additionally. </para>

        </sect2>

        <sect2>
            <title>jabberd Database Corruption</title>
            <para><literal>SYMPTOMS</literal>: After a disk is full error or a disk crash event, the
                jabberd database may have become corrupted. jabberd may then fail to start during
                spacewalk-service start:
                <screen> Starting spacewalk services...
   Initializing jabberd processes...
       Starting router                                                                   done
       Starting sm startproc:  exit status of parent of /usr/bin/sm: 2                   failed
   Terminating jabberd processes...</screen></para>

            <para>/var/log/messages shows more details:
                <screen>jabberd/sm[31445]: starting up
jabberd/sm[31445]: process id is 31445, written to /var/lib/jabberd/pid/sm.pid
jabberd/sm[31445]: loading 'db' storage module
jabberd/sm[31445]: db: corruption detected! close all jabberd processes and run db_recover
jabberd/router[31437]: shutting down</screen></para>

            <para><literal>CURE</literal>: Remove the jabberd database and restart. Jabberd will
                automatically re-create the database.
                <screen> spacewalk-service stop
 rm -Rf /var/lib/jabberd/db/*
 spacewalk-service start</screen>
            </para>


        </sect2>

    </sect1>



</chapter>
